{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxbn2yPygfHf"
      },
      "source": [
        "The Bloom_Filter functions takes our required input file and provides our desired output.\n",
        "\n",
        "To use it, call the function with:\n",
        "`lookup_count,FP_count=Bloom_Filter(inputfile,outputfile)`\n",
        "\n",
        "\n",
        "\n",
        "Note: In place of **inputfile** you can place your input csv file.\n",
        "\n",
        "**outputfile** will contain all the unique words of inputfile, with the duplicates removed. So thats our desired output.\n",
        "\n",
        "We need to put an empty .csv file at the desired location beforehand for **outputfile**.\n",
        "\n",
        "\n",
        "\n",
        "Also the function returns two output- **lookup_count** -> the number of lookups needed.\n",
        "\n",
        "**FP_count** -> the number of false positive by our algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "btU2BX4u42z6"
      },
      "outputs": [],
      "source": [
        "# Import required Libraries\n",
        "\n",
        "import csv\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pm6ODuNm5KbZ"
      },
      "outputs": [],
      "source": [
        "def Bloom_Filter(inp_filename,op_filename):\n",
        "\n",
        "     \n",
        "     \n",
        "     \n",
        "     lookup_count=0  # will store count of number of lookups\n",
        "     FP_count=0 # will store count of number of False Positives\n",
        "\n",
        "\n",
        "     \n",
        "     # Take our desired expected no. of element and probability of false positives from the users\n",
        "     exp_elem=float(input('What is the expected no. of Elements in our sample: '))\n",
        "     prob_false_pos=float(input('What is the probability of false positive: '))\n",
        "\n",
        "\n",
        "\n",
        "     # Calculating bit array side according to formula\n",
        "     bit_arr_size=round(-(exp_elem*np.log(prob_false_pos))/(np.square(np.log(2))))\n",
        "\n",
        "\n",
        "\n",
        "     # Calculating no. of hash functions according to formula\n",
        "     hash_func_num= round((bit_arr_size/exp_elem)*np.log(2))\n",
        "\n",
        "\n",
        "\n",
        "     # Generating the hash functions. Here we use a random function: (sum_of_ascii_values_of_word)%bit_arr_size+i , where i refers to the index from 0\n",
        "    # to no.of hash function  . Suppose a particular bloom filter is 4 bit-array, and 2 hash functions so\n",
        "    # Eg: H1-> ascii(\"Hello\")%4+0\n",
        "    #     H2 -> ascii(\"Hello\")%4+1\n",
        "     def filters_to_have():\n",
        "            lst_fltr=[]\n",
        "            for i in range(hash_func_num):\n",
        "               lst_fltr.append(char_val_sum%(bit_arr_size+i))\n",
        "        # Note this function creates the required no. of hash functions with the above formula. The frmula is random and there is scope of improvement\n",
        "        # If needed to add own formula, the user can delete one of the formulas from lst_fltr and add their own formula\n",
        "            return lst_fltr\n",
        "\n",
        "\n",
        "\n",
        "    # Create the bit array with the required size and fill them inititally with 0. We use Python list to create the bit array\n",
        "     bloom_fltr=[0]*bit_arr_size\n",
        "\n",
        "    \n",
        "\n",
        "    # we will read each and every word from .csv file one-by-one and apply the bloom filter\n",
        "     with open(inp_filename, 'r') as file:\n",
        "         my_reader = csv.reader(file, delimiter=',')\n",
        "         next(file)\n",
        "         for row in my_reader:\n",
        "              word=row[0]\n",
        "              char_val_sum=sum([ord(z) for z in list(word)]) # getting sum of ascii vals of characters\n",
        "              lst_fltr=filters_to_have() # generating the hash values for each word\n",
        "              lst_fltr=[len(bloom_fltr)-1  if i>len(bloom_fltr)-1 else i for i in lst_fltr ] # the index/hash value should not exceed max size of bit array so clipping the ones which are exceeding\n",
        "\n",
        "              # Bloom Filter below main logic to find out the words present\n",
        "\n",
        "              # checking how many 0's are obtained in bit array for each hash values\n",
        "              # suppose there are 4 hash values. If sum_all is 4 then all are 0's\n",
        "              sum_all=sum([bloom_fltr[i]==0 for i in lst_fltr])\n",
        "              # even a single 0 in bit array for any of the hash value will prove the word is not present/non-duplicate so we will add them in output file\n",
        "              if sum_all>0:\n",
        "                    with open(op_filename, 'a', newline='') as csvfile:\n",
        "                          spamwriter1 = csv.writer(csvfile, delimiter=' ',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "                          spamwriter1.writerow([word])\n",
        "                    for ij in lst_fltr:\n",
        "                        bloom_fltr[ij]=1\n",
        "              # if sum_all=0 then we need to lookup our outputfile to confirm that word is really present or it's a false positive\n",
        "              else:\n",
        "                    # lookup count increases by 1 since we are perfoming lookup\n",
        "                    lookup_count+=1\n",
        "                    # logic for checking whether the word is indeed present in the outputfile\n",
        "                    cc=0\n",
        "                    with open(op_filename, 'r') as file:\n",
        "                            my_reader = csv.reader(file, delimiter=',')\n",
        "                            next(file)\n",
        "                            for row1 in my_reader:\n",
        "                              word1=row1[0]\n",
        "\n",
        "                              if word==word1:\n",
        "                                  cc+=1\n",
        "\n",
        "                    if cc>0:\n",
        "                        pass\n",
        "                    else:\n",
        "                           # if the case in false positive(FP) increment the FP_count by 1\n",
        "                           FP_count+=1\n",
        "                           # since the case is FP so originally the word is not present in the outputfile, so add it there\n",
        "                           with open(op_filename, 'a', newline='') as csvfile:\n",
        "                                 spamwriter3 = csv.writer(csvfile, delimiter=' ',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "                                 spamwriter3.writerow([word])\n",
        "\n",
        "                    \n",
        "    \n",
        "\n",
        "\n",
        "     \n",
        "     \n",
        "     # Printing the lookup counts and false positive values\n",
        "     print('Number of False Positives: ', FP_count)\n",
        "     print('Number of Lookup Count: ', lookup_count)\n",
        "\n",
        "     # Returning the values of lookup counts and false positive\n",
        "     return [lookup_count,FP_count]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubHeQ4O6D_Ve",
        "outputId": "f8475ea7-c08d-441c-e246-6bf944938cb4"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'word_drop.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lookup_count,FP_count\u001b[39m=\u001b[39mBloom_Filter(\u001b[39m'\u001b[39;49m\u001b[39mstreaming_words.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mword_drop.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m------------\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLOOKUP COUNT \u001b[39m\u001b[39m'\u001b[39m,lookup_count)\n",
            "Cell \u001b[0;32mIn[2], line 74\u001b[0m, in \u001b[0;36mBloom_Filter\u001b[0;34m(inp_filename, op_filename)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m# logic for checking whether the word is indeed present in the outputfile\u001b[39;00m\n\u001b[1;32m     73\u001b[0m cc\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m---> 74\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(op_filename, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     75\u001b[0m         my_reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(file, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m         \u001b[39mnext\u001b[39m(file)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'word_drop.csv'"
          ]
        }
      ],
      "source": [
        "lookup_count,FP_count=Bloom_Filter('streaming_words.csv','word_drop.csv')\n",
        "\n",
        "print('------------')\n",
        "print('LOOKUP COUNT ',lookup_count)\n",
        "print('------------')\n",
        "print('FP_COUNT ',FP_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3ZhTmTRvWQx",
        "outputId": "bb80d247-3ebe-4c7e-df95-6455362cd9fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the expected no. of Elements in our sample: 7\n",
            "What is the probability of false positive: 0.05\n",
            "Number of False Positives:  2\n",
            "Number of Lookup Count:  7\n",
            "------------\n",
            "LOOKUP COUNT  7\n",
            "------------\n",
            "FP_COUNT  2\n"
          ]
        }
      ],
      "source": [
        "lookup_count,FP_count=Bloom_Filter('streaming_words.csv','word_drop.csv')\n",
        "\n",
        "print('------------')\n",
        "print('LOOKUP COUNT ',lookup_count)\n",
        "print('------------')\n",
        "print('FP_COUNT ',FP_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4ELr1VswPkD"
      },
      "source": [
        "Notice the difference in outputs in thw two above for different choice of expected no. of elements and FP Probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goycdCEtxQSJ"
      },
      "source": [
        "The unique words are saved in **word_drop.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqC3Q8di3LT3",
        "outputId": "94a9ad3c-75e5-4dd9-da55-9427d5d4f0ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the expected no. of Elements in our sample: 4\n",
            "What is the probability of false positive: 0.2\n",
            "Number of False Positives:  2\n",
            "Number of Lookup Count:  8\n"
          ]
        }
      ],
      "source": [
        "lookup_count,FP_count=Bloom_Filter('sample_test_bloom - Sheet6.csv','word_drop.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
